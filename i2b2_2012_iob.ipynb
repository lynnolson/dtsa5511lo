{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The i2b2 challenges do not provide data in the IOB format.  In this notebook we convert the 2012 dataset to that format so it can be fed into our models.\n",
    "\n",
    "Note: This notebook is a modification of the notebook from [ClinicalBERT github repo](https://github.com/EmilyAlsentzer/clinicalBERT/blob/master/downstream_tasks/i2b2_preprocessing/i2b2_2012/Reformat.ipynb).  For the most part this is *not* my code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert i2b2 2012 Data to IOB Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_CDATA = \"<TEXT><![CDATA[\"\n",
    "END_CDATA   = \"]]></TEXT>\"\n",
    "\n",
    "TAGS        = ['MEDICATION', 'OBSEE', 'SMOKER', 'HYPERTENSION', 'event', 'FAMILY_HIST']\n",
    "\n",
    "def read_xml_file(xml_path, event_tag_type='ALL_CHILDREN', match_text=True):\n",
    "    # print(xml_path)\n",
    "    with open(xml_path, mode='r') as f:\n",
    "        lines = f.readlines()\n",
    "        text, in_text = [], False\n",
    "        for i, l in enumerate(lines):\n",
    "            if START_CDATA in l:\n",
    "                text.append(list(l[l.find(START_CDATA) + len(START_CDATA):]))\n",
    "                in_text = True\n",
    "            elif END_CDATA in l:\n",
    "                text.append(list(l[:l.find(END_CDATA)]))\n",
    "                break\n",
    "            elif in_text:\n",
    "#                 if xml_path.endswith('180-03.xml') and '0808' in l and 'Effingham' in l:\n",
    "#                     print(\"Adjusting known error\")\n",
    "#                     l = l[:9] + ' ' * 4 + l[9:]\n",
    "# #                 elif xml_path.endswith('188-05.xml') and 'Johnson & Johnson' in l:\n",
    "# #                     print(\"Adjusting known error\")\n",
    "# #                     l = l.replace('&', 'and')\n",
    "                text.append(list(l))\n",
    "        \n",
    "    pos_transformer = {}\n",
    "    \n",
    "    linear_pos = 1\n",
    "    for line, sentence in enumerate(text):\n",
    "        for char_pos, char in enumerate(sentence):\n",
    "            pos_transformer[linear_pos] = (line, char_pos)\n",
    "            linear_pos += 1\n",
    "        \n",
    "    try: xml_parsed = ET.parse(xml_path)\n",
    "    except:\n",
    "        print(xml_path)\n",
    "        raise\n",
    "        \n",
    "    tag_containers = xml_parsed.findall('TAGS')\n",
    "    assert len(tag_containers) == 1, \"Found multiple tag sets!\"\n",
    "    tag_container = tag_containers[0]\n",
    "    \n",
    "#     event_tags = tag_container.getchildren() if event_tag_type == 'ALL_CHILDREN' else tag_container.findall('event')\n",
    "    event_tags = tag_container.findall('EVENT')\n",
    "    event_labels = [['O'] * len(sentence) for sentence in text]\n",
    "    for event_tag in event_tags:\n",
    "        base_label = event_tag.attrib['type']\n",
    "        start_pos, end_pos, event_text = event_tag.attrib['start'], event_tag.attrib['end'], event_tag.attrib['text']\n",
    "        start_pos, end_pos = int(start_pos)+1, int(end_pos)\n",
    "        event_text = ' '.join(event_text.split())\n",
    "#         if event_text == \"0808 O’neil’s Court\":\n",
    "#             print(\"Adjusting known error\")\n",
    "#             end_pos -= 4\n",
    "#         if event_text == 'Johnson and Johnson' and xml_path.endswith('188-05.xml'):\n",
    "#             print(\"Adjusting known error\")\n",
    "#             event_text = 'Johnson & Johnson'\n",
    "        \n",
    "\n",
    "        (start_line, start_char), (end_line, end_char) = pos_transformer[start_pos], pos_transformer[end_pos]\n",
    "            \n",
    "        obs_text = []\n",
    "        for line in range(start_line, end_line+1):\n",
    "            t = text[line]\n",
    "            s = start_char if line == start_line else 0\n",
    "            e = end_char if line == end_line else len(t)\n",
    "            obs_text.append(''.join(t[s:e+1]).strip())\n",
    "        obs_text = ' '.join(obs_text)\n",
    "        obs_text = ' '.join(obs_text.split())\n",
    "        \n",
    "        if '&apos;' in obs_text and '&apos;' not in event_text: event_text = event_text.replace(\"'\", \"&apos;\")\n",
    "        if '&quot;' in obs_text and '&quot;' not in event_text: event_text = event_text.replace('\"', '&quot;')\n",
    "              \n",
    "        if match_text: assert obs_text == event_text, (\n",
    "            (\"Texts don't match! %s v %s\" % (event_text, obs_text)) + '\\n' + str((\n",
    "                start_pos, end_pos, line, s, e, t, xml_path\n",
    "            ))\n",
    "        )\n",
    "            \n",
    "        if base_label.strip() == '': continue\n",
    "        \n",
    "        event_labels[end_line][end_char]     = 'I-%s' % base_label\n",
    "        event_labels[start_line][start_char] = 'B-%s' % base_label\n",
    "        \n",
    "        for line in range(start_line, end_line+1):\n",
    "            t = text[line]\n",
    "            s = start_char+1 if line == start_line else 0\n",
    "            e = end_char-1 if line == end_line else len(t)-1\n",
    "            for i in range(s, e+1): event_labels[line][i] = 'I-%s' % base_label\n",
    "\n",
    "    return text, event_labels\n",
    "    \n",
    "def merge_into_words(text_by_char, all_labels_by_char):\n",
    "    assert len(text_by_char) == len(all_labels_by_char), \"Incorrect # of sentences!\"\n",
    "    \n",
    "    N = len(text_by_char)\n",
    "    \n",
    "    text_by_word, all_labels_by_word = [], []\n",
    "    \n",
    "    for sentence_num in range(N):\n",
    "        sentence_by_char = text_by_char[sentence_num]\n",
    "        labels_by_char   = all_labels_by_char[sentence_num]\n",
    "        \n",
    "        assert len(sentence_by_char) == len(labels_by_char), \"Incorrect # of chars in sentence!\"\n",
    "        S = len(sentence_by_char)\n",
    "        \n",
    "        if labels_by_char == (['O'] * len(sentence_by_char)):\n",
    "            sentence_by_word = ''.join(sentence_by_char).split()\n",
    "            labels_by_word   = ['O'] * len(sentence_by_word)\n",
    "        else: \n",
    "            sentence_by_word, labels_by_word = [], []\n",
    "            text_chunks, labels_chunks = [], []\n",
    "            s = 0\n",
    "            for i in range(S):\n",
    "                if i == S-1:\n",
    "                    text_chunks.append(sentence_by_char[s:])\n",
    "                    labels_chunks.append(labels_by_char[s:])\n",
    "                elif labels_by_char[i] == 'O': continue\n",
    "                else:\n",
    "                    if i > 0 and labels_by_char[i-1] == 'O':\n",
    "                        text_chunks.append(sentence_by_char[s:i])\n",
    "                        labels_chunks.append(labels_by_char[s:i])\n",
    "                        s = i\n",
    "                    if labels_by_char[i+1] == 'O' or labels_by_char[i+1][2:] != labels_by_char[i][2:]:\n",
    "                        text_chunks.append(sentence_by_char[s:i+1])\n",
    "                        labels_chunks.append(labels_by_char[s:i+1])\n",
    "                        s = i+1\n",
    "                \n",
    "            for text_chunk, labels_chunk in zip(text_chunks, labels_chunks):\n",
    "                assert len(text_chunk) == len(labels_chunk), \"Bad Chunking (len)\"\n",
    "                assert len(text_chunk) > 0, \"Bad chunking (len 0)\" + str(text_chunks) + str(labels_chunks)\n",
    "                \n",
    "                labels_set = set(labels_chunk)\n",
    "                assert labels_set == set(['O']) or (len(labels_set) <= 3 and 'O' not in labels_set), (\n",
    "                    (\"Bad chunking (contents) %s\" % ', '.join(labels_set))+ str(text_chunks) + str(labels_chunks)\n",
    "                )\n",
    "                \n",
    "                text_chunk_by_word = ''.join(text_chunk).split()\n",
    "                W = len(text_chunk_by_word)\n",
    "                if W == 0: \n",
    "#                     assert labels_set == set(['O']), \"0-word chunking and non-0 label!\" + str(\n",
    "#                         text_chunks) + str(labels_chunks\n",
    "#                     )\n",
    "                    continue\n",
    "                \n",
    "                if labels_chunk[0] == 'O': labels_chunk_by_word = ['O'] * W\n",
    "                elif W == 1:               labels_chunk_by_word = [labels_chunk[0]]\n",
    "                elif W == 2:               labels_chunk_by_word = [labels_chunk[0], labels_chunk[-1]]\n",
    "                else:                      labels_chunk_by_word = [\n",
    "                        labels_chunk[0]\n",
    "                    ] + [labels_chunk[1]] * (W - 2) + [\n",
    "                        labels_chunk[-1]\n",
    "                    ]\n",
    "                    \n",
    "                sentence_by_word.extend(text_chunk_by_word)\n",
    "                labels_by_word.extend(labels_chunk_by_word)\n",
    "\n",
    "        assert len(sentence_by_word) == len(labels_by_word), \"Incorrect # of words in sentence!\"    \n",
    "        \n",
    "        if len(sentence_by_word) == 0: continue\n",
    "            \n",
    "        text_by_word.append(sentence_by_word)\n",
    "        all_labels_by_word.append(labels_by_word)\n",
    "    return text_by_word, all_labels_by_word\n",
    "\n",
    "def reprocess_event_labels(folders, base_path='.', event_tag_type='event', match_text=True, dev_set_size=None):\n",
    "    all_texts_by_patient, all_labels_by_patient = {}, {}\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_dir = os.path.join(base_path, folder)\n",
    "        xml_filenames = [x for x in os.listdir(folder_dir) if x.endswith('xml')]\n",
    "        for xml_filename in xml_filenames:\n",
    "            patient_num = int(xml_filename[:-4])\n",
    "            xml_filepath = os.path.join(folder_dir, xml_filename)\n",
    "            \n",
    "            text_by_char, labels_by_char = read_xml_file(\n",
    "                xml_filepath,\n",
    "                event_tag_type=event_tag_type,\n",
    "                match_text=match_text\n",
    "            )\n",
    "            text_by_word, labels_by_word = merge_into_words(text_by_char, labels_by_char)\n",
    "            \n",
    "            if patient_num not in all_texts_by_patient:\n",
    "                all_texts_by_patient[patient_num] = []\n",
    "                all_labels_by_patient[patient_num] = []\n",
    "            \n",
    "            all_texts_by_patient[patient_num].extend(text_by_word)\n",
    "            all_labels_by_patient[patient_num].extend(labels_by_word)\n",
    "            \n",
    "    patients = set(all_texts_by_patient.keys())\n",
    "    \n",
    "    if dev_set_size is None: train_patients, dev_patients = list(patients), []\n",
    "    else:\n",
    "        N_train = int(len(patients) * (1-dev_set_size))\n",
    "        patients_random = np.random.permutation(list(patients))\n",
    "        train_patients = list(patients_random[:N_train])\n",
    "        dev_patients   = list(patients_random[N_train:])\n",
    "    \n",
    "    train_texts, train_labels = [], []\n",
    "    dev_texts, dev_labels = [], []\n",
    "\n",
    "    print(f\"Number of train recs = {len(train_patients)}\")\n",
    "    print(f\"Number of dev recs = {len(dev_patients)}\")\n",
    "\n",
    "    train_doc_ids = []\n",
    "    for patient_num in train_patients:\n",
    "        for sent in all_texts_by_patient[patient_num]:\n",
    "            train_doc_ids.append([patient_num]*len(sent))\n",
    "        train_texts.extend(all_texts_by_patient[patient_num])\n",
    "        train_labels.extend(all_labels_by_patient[patient_num])\n",
    "\n",
    "    dev_doc_ids = []\n",
    "    for patient_num in dev_patients:\n",
    "        dev_texts.extend(all_texts_by_patient[patient_num])\n",
    "        for sent in all_texts_by_patient[patient_num]:\n",
    "            dev_doc_ids.append([patient_num]*len(sent))\n",
    "        dev_labels.extend(all_labels_by_patient[patient_num])\n",
    "\n",
    "    train_out_text_by_sentence = []\n",
    "    for doc_ids, text, labels in zip(train_doc_ids, train_texts, train_labels):\n",
    "        train_out_text_by_sentence.append('\\n'.join('%s %s %s' % x for x in zip(doc_ids, text, labels)))\n",
    "    dev_out_text_by_sentence = []\n",
    "    for doc_ids, text, labels in zip(dev_doc_ids, dev_texts, dev_labels):\n",
    "        dev_out_text_by_sentence.append('\\n'.join('%s %s %s' % x for x in zip(doc_ids, text, labels)))\n",
    "\n",
    "    return '\\n\\n'.join(train_out_text_by_sentence), '\\n\\n'.join(dev_out_text_by_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train recs = 171\n",
      "Number of dev recs = 19\n"
     ]
    }
   ],
   "source": [
    "final_train_text, final_dev_text = reprocess_event_labels(\n",
    "    ['data/i2b2/2012/2012-07-15.original-annotation.release'], dev_set_size=0.1, match_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train recs = 120\n",
      "Number of dev recs = 0\n"
     ]
    }
   ],
   "source": [
    "test_text, _ = reprocess_event_labels(\n",
    "    ['data/i2b2/2012/2012-08-08.test-data.event-timex-groundtruth/xml'], match_text=False, dev_set_size=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "120+171+19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1310674"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 Admission B-OCCURRENCE\n",
      "18 Date O\n",
      "18 : O\n",
      "\n",
      "18 2016-08-08 O\n",
      "\n",
      "18 Discharge B-OCCURRENCE\n",
      "18 Date O\n",
      "18 : O\n",
      "\n",
      "18 2016-08-15 O\n",
      "\n",
      "18 Discharge B-OCCURRENCE\n",
      "18 Date O\n",
      "18 : O\n",
      "\n",
      "18 2016-08-15 O\n",
      "\n",
      "18 HISTORY O\n",
      "18 OF O\n",
      "18 PRESENT O\n",
      "18 ILLNESS O\n",
      "18 : O\n",
      "\n",
      "18 The O\n",
      "18 patient O\n",
      "18 is O\n",
      "18 a O\n",
      "18 37 O\n",
      "18 year O\n",
      "18 old O\n",
      "18 lady O\n",
      "18 with O\n",
      "18 type B-PROBLEM\n",
      "18 1 I-PROBLEM\n",
      "18 diabetes I-PROBLEM\n",
      "18 mellitus I-PROBLEM\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(final_train_text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each split text to pandas frame and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_pandas_df(text):\n",
    "    docids, words, labels = [], [], []\n",
    "    for l in text.split('\\n'):\n",
    "        if len(l.strip()) > 0:\n",
    "            docid, word, label = l.split(' ')\n",
    "            assert len(docid.strip()) != 0\n",
    "            assert len(word.strip()) != 0\n",
    "            assert len(label.strip()) != 0\n",
    "            docids.append(docid)\n",
    "            words.append(word)\n",
    "            labels.append(label)\n",
    "    return pd.DataFrame({'docid': docids, 'word': words, 'NER_tag': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = convert_to_pandas_df(final_train_text)\n",
    "train_df.to_pickle('data/i2b2/2012/i2b2_train_dataset_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 ADMISSION B-OCCURRENCE\n",
      "311 DATE O\n",
      "311 : O\n",
      "\n",
      "311 04/07/97 O\n",
      "\n",
      "311 DISCHARGE B-OCCURRENCE\n",
      "311 DATE O\n",
      "311 : O\n",
      "\n",
      "311 04/08/97 O\n",
      "\n",
      "311 HISTORY O\n",
      "311 OF O\n",
      "311 PRESENT O\n",
      "311 ILLNESS O\n",
      "311 : O\n",
      "\n",
      "311 Mr. O\n",
      "311 Vessels O\n",
      "311 is O\n",
      "311 a O\n",
      "311 49-year-old O\n",
      "311 man O\n",
      "311 status O\n",
      "311 post O\n",
      "311 orthotopic B-TREATMENT\n",
      "311 heart I-TREATMENT\n",
      "311 transplantation I-TREATMENT\n",
      "311 in O\n",
      "311 1991 O\n",
      "311 at O\n",
      "311 Dauteno\n"
     ]
    }
   ],
   "source": [
    "print(final_dev_text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = convert_to_pandas_df(final_dev_text)\n",
    "dev_df.to_pickle('data/i2b2/2012/i2b2_dev_dataset_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516 ADMISSION B-OCCURRENCE\n",
      "516 DATE O\n",
      "516 : O\n",
      "\n",
      "516 10/14/96 O\n",
      "\n",
      "516 DISCHARGE B-OCCURRENCE\n",
      "516 DATE O\n",
      "516 : O\n",
      "\n",
      "516 10/27/96 O\n",
      "516 date O\n",
      "516 of O\n",
      "516 birth B-OCCURRENCE\n",
      "516 ; O\n",
      "516 September O\n",
      "516 30 O\n",
      "516 , O\n",
      "516 1917 O\n",
      "\n",
      "516 THER O\n",
      "516 PROCEDURES O\n",
      "516 : O\n",
      "\n",
      "516 arterial B-TEST\n",
      "516 catheterization I-TEST\n",
      "516 on O\n",
      "516 10/14/96 O\n",
      "516 , O\n",
      "516 head B-TEST\n",
      "516 CT I-TEST\n",
      "516 scan I-TEST\n",
      "516 on O\n",
      "516 10/1\n"
     ]
    }
   ],
   "source": [
    "print(test_text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = convert_to_pandas_df(test_text)\n",
    "test_df.to_pickle('data/i2b2/2012/i2b2_test_dataset_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {}\n",
    "for s in final_train_text, final_dev_text, test_text:\n",
    "    for line in s.split('\\n'):\n",
    "        if line == '': continue\n",
    "        label = line.split()[-1]\n",
    "        assert label == 'O' or label.startswith('B-') or label.startswith('I-'), \"label wrong! %s\" % label\n",
    "        if label not in labels: labels[label] = 1\n",
    "        else: labels[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-OCCURRENCE': 5774,\n",
       " 'O': 114910,\n",
       " 'B-PROBLEM': 9319,\n",
       " 'I-PROBLEM': 13543,\n",
       " 'B-TREATMENT': 7098,\n",
       " 'I-TREATMENT': 6748,\n",
       " 'I-OCCURRENCE': 3590,\n",
       " 'B-EVIDENTIAL': 1334,\n",
       " 'B-TEST': 4762,\n",
       " 'I-TEST': 5931,\n",
       " 'I-EVIDENTIAL': 84,\n",
       " 'B-CLINICAL_DEPT': 1724,\n",
       " 'I-CLINICAL_DEPT': 3253}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/i2b2/2012/processed/train.tsv', mode='w') as f:\n",
    "#     f.write(final_train_text)\n",
    "# with open('data/i2b2/2012/processed/dev.tsv', mode='w') as f:\n",
    "#     f.write(final_dev_text)\n",
    "# with open('data/i2b2/2012/processed/test.tsv', mode='w') as f:\n",
    "#     f.write(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.concat([train_df, dev_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_pickle('data/i2b2/2012/i2b2_dataset_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
